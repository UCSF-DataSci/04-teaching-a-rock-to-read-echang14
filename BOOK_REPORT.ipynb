{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466204de",
   "metadata": {},
   "source": [
    "# The Lazy Book Report\n",
    "\n",
    "Your professor has assigned a book report on \"The Red-Headed League\" by Arthur Conan Doyle. \n",
    "\n",
    "You haven't read the book. And out of stubbornness, you won't.\n",
    "\n",
    "But you *have* learned NLP. Let's use it to answer the professor's questions without reading.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's fetch the text from Project Gutenberg and prepare it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08a46884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story loaded: 4000 words in 3 sections\n",
      "Section sizes: [1333, 1333, 1334]\n"
     ]
    }
   ],
   "source": [
    "# Fetch and prepare text - RUN THIS CELL FIRST\n",
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "url = 'https://www.gutenberg.org/files/1661/1661-0.txt'\n",
    "req = urllib.request.Request(url, headers={'User-Agent': 'Python-urllib'})\n",
    "with urllib.request.urlopen(req, timeout=30) as resp:\n",
    "    text = resp.read().decode('utf-8')\n",
    "\n",
    "# Strip Gutenberg boilerplate\n",
    "text = text.split('*** START OF')[1].split('***')[1]\n",
    "text = text.split('*** END OF')[0]\n",
    "\n",
    "# Extract \"The Red-Headed League\" story (it's the second story in the collection)\n",
    "matches = list(re.finditer(r'THE RED-HEADED LEAGUE', text, re.IGNORECASE))\n",
    "story_start = matches[1].end()\n",
    "story_text = text[story_start:]\n",
    "story_end = re.search(r'\\n\\s*III\\.\\s*\\n', story_text)\n",
    "story_text = story_text[:story_end.start()] if story_end else story_text\n",
    "\n",
    "# Split into 3 sections by word count\n",
    "words = story_text.split()[:4000]\n",
    "section_size = len(words) // 3\n",
    "sections = [\n",
    "    ' '.join(words[:section_size]),\n",
    "    ' '.join(words[section_size:2*section_size]),\n",
    "    ' '.join(words[2*section_size:])\n",
    "]\n",
    "\n",
    "print(f\"Story loaded: {len(words)} words in {len(sections)} sections\")\n",
    "print(f\"Section sizes: {[len(s.split()) for s in sections]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e757a",
   "metadata": {},
   "source": [
    "## Professor's Questions\n",
    "\n",
    "Your professor wants you to answer 5 questions about the story. Let's use NLP to find the answers.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 1: Writing Style\n",
    "\n",
    "> \"This text is from the 1890s. What makes it different from modern writing?\"\n",
    "\n",
    "**NLP Method:** Use preprocessing to compute text statistics. Tokenize the text and calculate:\n",
    "- Vocabulary richness (unique words / total words)\n",
    "- Average sentence length\n",
    "- Average word length\n",
    "\n",
    "**Hint:** Formal, literary writing typically shows higher vocabulary richness and longer sentences than modern casual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1710e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: compute text statistics\n",
    "# You'll need: import string, import re\n",
    "# - Tokenize: remove punctuation, lowercase\n",
    "# - Sentences: split on sentence-ending punctuation\n",
    "# Calculate vocab_richness, avg_sentence_length, avg_word_length\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "def compute_text_statistics(text):\n",
    "    text_clean = text.lower()\n",
    "    text_clean = text_clean.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = text_clean.split()\n",
    "    total_words = len(words)\n",
    "    unique_words = len(set(words))\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s for s in sentences if s.strip()]\n",
    "    vocab_richness = unique_words / total_words if total_words > 0 else 0\n",
    "    avg_sentence_length = total_words / len(sentences) if len(sentences)> 0 else 0\n",
    "    avg_word_length = sum(len(word) for word in words) / total_words if total_words > 0 else 0\n",
    "    return {\n",
    "        'vocab_richness': vocab_richness,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_word_length': avg_word_length}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5545405",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2: Main Characters\n",
    "\n",
    "> \"Who are the main characters in this story?\"\n",
    "\n",
    "**NLP Method:** Use Named Entity Recognition (NER) to extract PERSON entities.\n",
    "\n",
    "**Hint:** Use spaCy's `en_core_web_sm` model. Process the text and filter entities where `ent.label_ == 'PERSON'`. Count how often each name appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "332a59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: extract PERSON entities using spaCy NER\n",
    "# You'll need: import spacy, nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/characters.txt\", \"w\") as f:\n",
    "#     for name in your_character_list:\n",
    "#         f.write(f\"{name}\\n\")\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def extract_person_entities(text):\n",
    "    doc = nlp(text)\n",
    "    persons = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            persons.add(ent.text)\n",
    "    return list(persons)\n",
    "\n",
    "with open(\"output/characters.txt\", \"w\") as f:\n",
    "    characters = extract_person_entities(story_text)\n",
    "    for name in characters:\n",
    "        f.write(f\"{name}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732e661",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3: Story Locations\n",
    "\n",
    "> \"Where does the story take place?\"\n",
    "\n",
    "**NLP Method:** Use Named Entity Recognition (NER) to extract location entities (GPE and LOC).\n",
    "\n",
    "**Hint:** Filter entities where `ent.label_` is 'GPE' (geopolitical entity) or 'LOC' (location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3f8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: extract GPE and LOC entities using spaCy NER\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/locations.txt\", \"w\") as f:\n",
    "#     for place in your_locations_list:\n",
    "#         f.write(f\"{place}\\n\")\n",
    "\n",
    "def extract_location_entities(text):\n",
    "    doc = nlp(text)\n",
    "    locations = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "            locations.add(ent.text)\n",
    "    return list(locations)\n",
    "\n",
    "with open(\"output/locations.txt\", \"w\") as f:\n",
    "    locations = extract_location_entities(story_text)\n",
    "    for place in locations:\n",
    "        f.write(f\"{place}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b228d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4: Wilson's Business\n",
    "\n",
    "> \"What is Wilson's business?\"\n",
    "\n",
    "**NLP Method:** Use TF-IDF similarity to find which section discusses Wilson's business.\n",
    "\n",
    "**Hint:** Create a TF-IDF vectorizer, fit it on the 3 sections, then transform your query using the same vectorizer (`.transform()`, not `.fit_transform()` - you want to use the vocabulary learned from the sections). Find which section has the highest cosine similarity and read it to find the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f7fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: use TF-IDF similarity to find the relevant section\n",
    "# You'll need: from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#              from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/business.txt\", \"w\") as f:\n",
    "#     f.write(\"Wilson's business is: ...\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_relevant_section(sections, query):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sections + [query])\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    most_relevant_index = cosine_similarities.argmax()\n",
    "    return sections[most_relevant_index]\n",
    "\n",
    "with open(\"output/business.txt\", \"w\") as f:\n",
    "    query = \"Wilson's business is\"\n",
    "    relevant_section = find_relevant_section(sections, query)\n",
    "    f.write(relevant_section)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85452bf1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 5: Wilson's Work Routine\n",
    "\n",
    "> \"What is Wilson's daily work routine for the League?\"\n",
    "\n",
    "**NLP Method:** Use TF-IDF similarity to find which section discusses Wilson's work routine.\n",
    "\n",
    "**Hint:** Similar to Question 4 - use TF-IDF to find the section that best matches your query about work routine. The answer includes what Wilson had to do and what eventually happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ddea14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: use TF-IDF similarity to find the relevant section\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/routine.txt\", \"w\") as f:\n",
    "#     f.write(\"Wilson's work routine: ...\\n\")\n",
    "#     f.write(\"What happened: ...\\n\")\n",
    "\n",
    "query = \"What is Wilson's daily work routine for the League and what happened?\"\n",
    "\n",
    "best_section = find_relevant_section(sections, query)\n",
    "\n",
    "with open(\"output/routine.txt\", \"w\") as f:\n",
    "    f.write(f\"Wilson's work routine and what happened: {best_section}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
